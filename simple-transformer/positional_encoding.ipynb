{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1: Input embedding \n",
    "\n",
    "original sentence:\n",
    "\n",
    "    - LEARNT token embeddings (vector) of size 512 - values between (0, 1)\n",
    "\n",
    "    - FIXED positional embedding of size 512\n",
    "\n",
    "        - calculated using sine and cosine functions of position and dimension to capture unique, periodic patterns\n",
    "\n",
    "    - encoder input => token embeddings + positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "vocab_size = 6           # max_seq_len\n",
    "embedding_size = 8       # d_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randn creates a (no of lines - here 3) dim tensor of vocab_size x embedding_size\n",
    "torch.manual_seed(132)\n",
    "token_embed = torch.randn(batch_size, vocab_size, embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5628, -0.0576, -0.8741,  0.0612, -0.1741,  2.1812,  0.1573,\n",
       "          -1.4425],\n",
       "         [ 2.1285,  0.3479,  0.9685,  0.8488,  0.5011,  0.9357,  0.4584,\n",
       "           1.2065],\n",
       "         [-0.7349, -0.9706, -1.2097,  1.6960,  1.4241, -0.5467, -0.4898,\n",
       "          -1.6538],\n",
       "         [ 1.0423,  0.6688, -1.0684,  0.2520, -1.9652,  0.3251, -0.2690,\n",
       "          -1.0828],\n",
       "         [ 0.1590,  1.0780, -1.4566, -0.8454, -0.1251,  2.0603,  0.7509,\n",
       "           0.0553],\n",
       "         [ 0.8920,  0.7346, -1.5393,  0.7653, -0.4533,  2.4488, -0.5876,\n",
       "          -1.3040]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: Positional encoding\n",
    "\n",
    "- generate an empty tensor 'pe' of size: embedding_size for each word in vocab\n",
    "- create another column vector 'positions' which holds the index of every word in vocab\n",
    "    - unsqueeze turns 1D row tensor to a 2D column tensor\n",
    "- for loop to iterate through each vocab and it's embeddings \n",
    "    - calculate denominator first\n",
    "    - calculate even and odd positional embedding for each pe\n",
    "\n",
    "- final positional encoding => position_encod = position + pe\n",
    "- final embeddings => positional_embed = position_encod + token_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.zeros(vocab_size, embedding_size).float()\n",
    "pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, vocab_size).unsqueeze(1).float()\n",
    "position.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in range(vocab_size):\n",
    "    for i in range(embedding_size):\n",
    "        #print(pos, i)\n",
    "        den = 10000 ** (2 * i / embedding_size)\n",
    "        if (i % 2 == 0):\n",
    "            pe[pos, i] = np.sin(pos/ den)\n",
    "        else:\n",
    "            pe[pos, i] = np.cos(pos/ den)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  9.9500e-01,  9.9998e-03,  1.0000e+00,  1.0000e-04,\n",
       "          1.0000e+00,  1.0000e-06,  1.0000e+00],\n",
       "        [ 9.0930e-01,  9.8007e-01,  1.9999e-02,  1.0000e+00,  2.0000e-04,\n",
       "          1.0000e+00,  2.0000e-06,  1.0000e+00],\n",
       "        [ 1.4112e-01,  9.5534e-01,  2.9996e-02,  1.0000e+00,  3.0000e-04,\n",
       "          1.0000e+00,  3.0000e-06,  1.0000e+00],\n",
       "        [-7.5680e-01,  9.2106e-01,  3.9989e-02,  9.9999e-01,  4.0000e-04,\n",
       "          1.0000e+00,  4.0000e-06,  1.0000e+00],\n",
       "        [-9.5892e-01,  8.7758e-01,  4.9979e-02,  9.9999e-01,  5.0000e-04,\n",
       "          1.0000e+00,  5.0000e-06,  1.0000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embed = pe + token_embed\n",
    "positional_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5628,  0.9424, -0.8741,  1.0612, -0.1741,  3.1812,  0.1573,\n",
       "          -0.4425],\n",
       "         [ 2.9700,  1.3429,  0.9785,  1.8488,  0.5012,  1.9357,  0.4584,\n",
       "           2.2065],\n",
       "         [ 0.1744,  0.0095, -1.1897,  2.6960,  1.4243,  0.4533, -0.4898,\n",
       "          -0.6538],\n",
       "         [ 1.1834,  1.6242, -1.0384,  1.2520, -1.9649,  1.3251, -0.2690,\n",
       "          -0.0828],\n",
       "         [-0.5978,  1.9990, -1.4166,  0.1546, -0.1247,  3.0603,  0.7509,\n",
       "           1.0553],\n",
       "         [-0.0670,  1.6122, -1.4893,  1.7653, -0.4528,  3.4488, -0.5876,\n",
       "          -0.3040]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class implementation\n",
    "\n",
    "torch.manual_seed(132)\n",
    "token_embed = torch.randn(batch_size, vocab_size, embedding_size).to(device)\n",
    "\n",
    "class PosEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        #self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.token_embed = token_embed.to(device)        \n",
    "        self.pos_embed = self.pos_encod(max_len, d_model)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def pos_encod(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(d_model):\n",
    "                den = 10000 ** (2 * i / d_model)\n",
    "                if (i % 2 == 0):\n",
    "                    pe[pos, i] = torch.sin(torch.tensor(pos, dtype=torch.float, device=device) / den)\n",
    "                else:\n",
    "                    pe[pos, i] = torch.cos(torch.tensor(pos, dtype=torch.float, device=device) / den)\n",
    "        \n",
    "        return pe.unsqueeze(0).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.token_embed + self.pos_embed[:, :x.size(1), :]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PosEmbedding(vocab_size = 6, d_model = 8, max_len = 1).to(device)\n",
    "x = torch.arange(10).unsqueeze(0).to(device)\n",
    "output = model(x)\n",
    "\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5628,  0.9424, -0.8741,  1.0612, -0.1741,  3.1812,  0.1573,\n",
       "          -0.4425],\n",
       "         [ 2.1285,  1.3479,  0.9685,  1.8488,  0.5011,  1.9357,  0.4584,\n",
       "           2.2065],\n",
       "         [-0.7349,  0.0294, -1.2097,  2.6960,  1.4241,  0.4533, -0.4898,\n",
       "          -0.6538],\n",
       "         [ 1.0423,  1.6688, -1.0684,  1.2520, -1.9652,  1.3251, -0.2690,\n",
       "          -0.0828],\n",
       "         [ 0.1590,  2.0780, -1.4566,  0.1546, -0.1251,  3.0603,  0.7509,\n",
       "           1.0553],\n",
       "         [ 0.8920,  1.7346, -1.5393,  1.7653, -0.4533,  3.4488, -0.5876,\n",
       "          -0.3040]]], device='mps:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
