{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "vocab_size = 6           # max_seq_len\n",
    "embedding_size = 8       # d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(132)\n",
    "positional_embed = torch.randn(batch_size, vocab_size, embedding_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3: Self Attention & Multi-Head Attention\n",
    "\n",
    "- Input embeddings capture meaning of the word; positonal encoding captures positional significance\n",
    "\n",
    "- Now self-attention captures relation of words with each other (How?)\n",
    "\n",
    "    - Queries Q: “What am I looking for?”\n",
    "    - Keys K: “What do others offer?”\n",
    "    - Values V: “What do I take from them?”\n",
    "\n",
    "- Each of these are initialized as a separate weight matrix then multiplied with the original embedding matrix (with pe)\n",
    "\n",
    "- To compute attention score:\n",
    "    - calculate q.kT -> kT ensures that each token attends to every other token in the sequence\n",
    "    - divide by sqrt(d_model) -> this scales the values to prevent overly large attention scores, which can lead to unstable gradients\n",
    "    - take softmax -> normalizes attention score\n",
    "\n",
    "- Multi-head attention is simply self attention applied parallely to all heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8\n",
    "d_model = 8 # embedding_size\n",
    "heads = 2 # change for multi-head attention\n",
    "\n",
    "#dk, dv = 6, 6      # dk = d_model / number of heads \n",
    "dk = d_model // heads  \n",
    "dv = d_model // heads  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq = torch.randn(d_model, dk)  # size: 8, 4\n",
    "wk = torch.randn(d_model, dk)\n",
    "wv = torch.randn(d_model, dv)\n",
    "\n",
    "wq.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7588, -0.5084, -3.7370,  2.5728],\n",
       "         [ 2.2291, -1.9921, -0.0241,  0.0073],\n",
       "         [ 2.6539,  4.3607, -3.9289, -1.5583],\n",
       "         [-1.2607,  1.8096,  4.1857,  3.5735],\n",
       "         [-0.2976,  0.2599, -0.8966,  3.9972],\n",
       "         [ 2.5838,  0.4920, -1.7484,  5.4579]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = positional_embed @ wq\n",
    "q  # ([1, 6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.2813, -1.4716, -0.8473,  1.1307],\n",
       "         [ 1.3028, -3.2291,  3.8855, -1.2693],\n",
       "         [-5.6884,  1.1877,  0.5104,  1.7375],\n",
       "         [ 0.3116, -3.2665,  1.4287,  1.8076],\n",
       "         [ 0.0575, -1.8384,  2.4462,  2.0588],\n",
       "         [-4.2674, -2.8822,  0.9956,  2.5251]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = positional_embed @ wk\n",
    "k   # ([1, 6, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7858,  0.2538,  3.8358, -0.5133],\n",
       "         [ 4.3383,  0.0158,  0.9234,  4.0296],\n",
       "         [ 2.5303,  0.5589,  1.3418, -0.1750],\n",
       "         [ 4.1207, -2.6188,  2.8017, -0.8283],\n",
       "         [-0.6290,  1.1233,  3.1431, -2.9573],\n",
       "         [ 4.9291, -0.6216,  4.2550,  0.6335]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = positional_embed @ wv\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 6])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkT = q @ k.transpose(-2, -1)\n",
    "qkT.size()  # size: [1, 6, 6] @ [6, 6, 1] = [1, 6, 5]\n",
    "\n",
    "#qkT = torch.matmul(q, k.transpose(-2, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = qkT / np.sqrt(dk)\n",
    "score.size()    # [1, 6, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 6])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.softmax(score, dim = -1) # dim=-1 normalizes across keys (last dim), so each row sums to 1\n",
    "softmax.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.9723e-01, 5.1160e-05, 3.0764e-02, 1.8298e-01, 2.3849e-02,\n",
       "          1.6512e-01],\n",
       "         [2.5808e-04, 7.0204e-01, 3.7513e-06, 2.5148e-01, 4.5167e-02,\n",
       "          1.0501e-03],\n",
       "         [2.9461e-01, 6.2736e-03, 6.4987e-01, 1.7608e-02, 3.1512e-02,\n",
       "          1.2517e-04],\n",
       "         [5.6472e-04, 9.3755e-04, 7.7077e-01, 2.4161e-03, 1.3603e-01,\n",
       "          8.9283e-02],\n",
       "         [8.7890e-02, 3.0153e-05, 2.8013e-01, 4.9009e-02, 6.4149e-02,\n",
       "          5.1879e-01],\n",
       "         [2.5369e-03, 5.1094e-05, 1.2667e-03, 5.3412e-01, 4.4564e-01,\n",
       "          1.6383e-02]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = softmax @ v\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.2947, -0.3863,  3.6224, -0.4292],\n",
       "         [ 4.0594, -0.5974,  1.5002,  2.4876],\n",
       "         [ 2.5456,  0.4273,  2.1568, -0.3474],\n",
       "         [ 2.3204,  0.5219,  1.8515, -0.4792],\n",
       "         [ 3.6725, -0.1999,  3.2594,  0.0043],\n",
       "         [ 2.0119, -0.9070,  2.9783, -1.7512]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row sums: tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "row_sums = softmax.sum(dim=-1)  # sums along the last dimension\n",
    "print(\"Row sums:\", row_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# org\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dk, dv):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(d_model, dk)  \n",
    "        self.wk = nn.Linear(d_model, dk)  \n",
    "        self.wv = nn.Linear(d_model, dv)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.wq(x)  \n",
    "        k = self.wk(x)  \n",
    "        v = self.wv(x) \n",
    "\n",
    "        return self.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1))  # computes dot-product attention scores\n",
    "        dk = q.size(-1)  # gets embedding dimension\n",
    "        scores = scores / torch.sqrt(torch.tensor(float(dk)))  # normalizes scores\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)  \n",
    "        output = torch.matmul(weights, v)  # computes weighted sum \n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dk, dv):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(d_model, dk)  \n",
    "        self.wk = nn.Linear(d_model, dk)  \n",
    "        self.wv = nn.Linear(d_model, dv)  \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        print(f\"Input x shape: {x.shape}\")  \n",
    "\n",
    "        q = self.wq(x)  \n",
    "        k = self.wk(x)  \n",
    "        v = self.wv(x) \n",
    "\n",
    "        print(f\"Query (q) shape: {q.shape}\")  \n",
    "        print(f\"Key (k) shape: {k.shape}\")  \n",
    "        print(f\"Value (v) shape: {v.shape}\") \n",
    "\n",
    "        return self.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        \"\"\"\n",
    "        q, k: [batch_size, seq_len, dk]\n",
    "        v: [batch_size, seq_len, dv]\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1))  \n",
    "        print(f\"Scores shape (q @ k.T): {scores.shape}\")  #\n",
    "\n",
    "        dk = q.size(-1) \n",
    "        scores = scores / torch.sqrt(torch.tensor(float(dk))) \n",
    "        print(f\"Scaled scores shape: {scores.shape}\") \n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)  \n",
    "        print(f\"Softmax weights shape: {weights.shape}\") \n",
    "\n",
    "        output = torch.matmul(weights, v)  \n",
    "        print(f\"Output shape (weights @ v): {output.shape}\") \n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x shape: torch.Size([1, 6, 8])\n",
      "Query (q) shape: torch.Size([1, 6, 6])\n",
      "Key (k) shape: torch.Size([1, 6, 6])\n",
      "Value (v) shape: torch.Size([1, 6, 6])\n",
      "Scores shape (q @ k.T): torch.Size([1, 6, 6])\n",
      "Scaled scores shape: torch.Size([1, 6, 6])\n",
      "Softmax weights shape: torch.Size([1, 6, 6])\n",
      "Output shape (weights @ v): torch.Size([1, 6, 6])\n",
      "Output: tensor([[[ 0.1561, -0.2973,  0.4194, -0.5169, -0.2889, -0.0047],\n",
      "         [ 0.2791, -0.4766,  0.5885, -0.7013, -0.4492,  0.0928],\n",
      "         [ 0.1080, -0.2189,  0.3061, -0.4617, -0.2391, -0.0028],\n",
      "         [ 0.1573, -0.2738,  0.4379, -0.4548, -0.2977, -0.0793],\n",
      "         [ 0.1876, -0.2317,  0.3680, -0.5485, -0.3321,  0.0698],\n",
      "         [ 0.1673, -0.2950,  0.4363, -0.5147, -0.3023, -0.0130]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Output shape: torch.Size([1, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "model = SelfAttention(d_model = 8, dk = 6, dv = 6)\n",
    "\n",
    "attention = model(positional_embed)\n",
    "\n",
    "print(\"Output:\", attention)\n",
    "print(\"\\nOutput shape:\", attention.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
