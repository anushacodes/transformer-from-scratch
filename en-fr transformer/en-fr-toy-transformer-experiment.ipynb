{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = \"this is a fat cat\"      \n",
    "fr_sentence = \"c'est un chat gros\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SOS = \"<sos>\" # start of sentence\n",
    "EOS = \"<eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: tokenisation\n",
    " - we create a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "en_tokens = en_sentence.split() \n",
    "fr_tokens = [SOS] + fr_sentence.split() + [EOS]  # adding SOS and EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en = {word: idx for idx, word in enumerate((en_tokens))}\n",
    "\n",
    "# for repeated words, use set instead\n",
    "# {word: idx for idx, word in enumerate(set(en_tokens))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0, 'is': 1, 'a': 2, 'fat': 3, 'cat': 4}"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fr = {word: idx for idx, word in enumerate((fr_tokens))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<sos>': 0, \"c'est\": 1, 'un': 2, 'chat': 3, 'gros': 4, '<eos>': 5}"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: converting each token into an embedding vector\n",
    " - let's consider d_model = 6 for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embed = torch.randn(len(vocab_en), d_model)\n",
    "\n",
    "en_embed.size() # should be 5 x 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_embed = torch.randn(len(vocab_fr), d_model)\n",
    "\n",
    "fr_embed.size() # should be 6 x 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: adding positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = len(vocab_en)   # length of input seq of en = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create positional encoding (sin/cos formula)\n",
    "def pos_encoding(seq_len, d_model, embed):\n",
    "\n",
    "    pos_encod = np.zeros((seq_len, d_model)) # np.zeros expects a tuple for shape\n",
    "\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(d_model):\n",
    "            if i % 2 == 0:\n",
    "                pos_encod[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            else:\n",
    "                pos_encod[pos, i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "    \n",
    "    return pos_encod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.04639922,  0.99892298,  0.00215443,\n",
       "         0.99999768],\n",
       "       [ 0.90929743, -0.41614684,  0.0926985 ,  0.99569422,  0.00430886,\n",
       "         0.99999072],\n",
       "       [ 0.14112001, -0.9899925 ,  0.1387981 ,  0.9903207 ,  0.00646326,\n",
       "         0.99997911],\n",
       "       [-0.7568025 , -0.65364362,  0.18459872,  0.98281398,  0.00861763,\n",
       "         0.99996287],\n",
       "       [-0.95892427,  0.28366219,  0.23000171,  0.97319022,  0.01077197,\n",
       "         0.99994198]])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_pos_encod = pos_encoding(6, 6, fr_embed)\n",
    "fr_pos_encod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target (french) embeddings with positional encoding:\n",
      " tensor([[-1.8414, -0.1653, -1.6628, -0.9757,  0.5443,  2.3233],\n",
      "        [ 0.7097,  0.8202,  0.1538,  1.2056, -1.7091,  1.3063],\n",
      "        [ 0.9607, -0.7560,  1.3283,  0.2207, -0.3546,  0.3512],\n",
      "        [-1.1170, -2.5425,  0.0085,  0.8106,  0.2896, -0.6482],\n",
      "        [-1.7519, -2.2727, -0.0269,  2.0114, -0.7436,  1.1899],\n",
      "        [-2.2542,  0.8670,  0.3996, -0.7127, -0.0201, -0.3976]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/jcq2q4656l14lwb2cdw1pkfm0000gn/T/ipykernel_31576/763448820.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fr_embeddings_with_position = torch.tensor(fr_embeddings_with_position, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# adding positional encoding to french embeddings\n",
    "fr_embeddings_with_position = fr_embed + fr_pos_encod\n",
    "fr_embeddings_with_position = torch.tensor(fr_embeddings_with_position, dtype=torch.float32)\n",
    "\n",
    "print(\"target (french) embeddings with positional encoding:\\n\", fr_embeddings_with_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.04639922,  0.99892298,  0.00215443,\n",
       "         0.99999768],\n",
       "       [ 0.90929743, -0.41614684,  0.0926985 ,  0.99569422,  0.00430886,\n",
       "         0.99999072],\n",
       "       [ 0.14112001, -0.9899925 ,  0.1387981 ,  0.9903207 ,  0.00646326,\n",
       "         0.99997911],\n",
       "       [-0.7568025 , -0.65364362,  0.18459872,  0.98281398,  0.00861763,\n",
       "         0.99996287]])"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_pos_encod = pos_encoding(5, 6, en_embed)\n",
    "en_pos_encod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (en) embeddings with positional encoding:\n",
      " tensor([[-0.0771,  1.1392, -0.2497, -0.5263, -0.7974,  0.4814],\n",
      "        [ 0.1057,  0.4611, -1.2704,  0.6989,  1.0642,  0.7467],\n",
      "        [ 2.3867, -0.5689,  0.2371,  1.6630, -1.0370,  2.0919],\n",
      "        [ 0.2475, -2.0977, -0.2304,  1.3468, -0.8500,  2.1827],\n",
      "        [-1.8206, -2.3345,  0.9096,  0.3176, -1.5079,  1.5184]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/jcq2q4656l14lwb2cdw1pkfm0000gn/T/ipykernel_31576/1949586055.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  en_embeddings_with_position = torch.tensor(en_embeddings_with_position, dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# adding positional encoding to english embeddings\n",
    "en_embeddings_with_position = en_embed + en_pos_encod\n",
    "en_embeddings_with_position = torch.tensor(en_embeddings_with_position, dtype = torch.float32)\n",
    "\n",
    "print(\"input (en) embeddings with positional encoding:\\n\", en_embeddings_with_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/jcq2q4656l14lwb2cdw1pkfm0000gn/T/ipykernel_31576/2887565307.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fr_embed_tensor = torch.tensor(fr_embeddings_with_position, dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "fr_embed_tensor = torch.tensor(fr_embeddings_with_position, dtype = torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Masked self-attention for the decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_seq_len = len(vocab_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a triangular matrix wwith lower half as 1 rest 0 (a mask for future words)\n",
    "mask = np.tril(np.ones((fr_seq_len, fr_seq_len))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.e+00, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09],\n",
       "       [ 0.e+00,  0.e+00, -1.e+09, -1.e+09, -1.e+09, -1.e+09],\n",
       "       [ 0.e+00,  0.e+00,  0.e+00, -1.e+09, -1.e+09, -1.e+09],\n",
       "       [ 0.e+00,  0.e+00,  0.e+00,  0.e+00, -1.e+09, -1.e+09],\n",
       "       [ 0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00, -1.e+09],\n",
       "       [ 0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -1e9 #-np.infty\n",
    "mask[mask == 1] = 0\n",
    "mask   # -inf = no context whatsoever\n",
    "# tho infinity shouldn't be used, we use a very small number (1e-9) for a working transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8414e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [ 7.0966e-01,  8.2017e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [ 9.6071e-01, -7.5597e-01,  1.3283e+00, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-1.1170e+00, -2.5425e+00,  8.4663e-03,  8.1057e-01, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-1.7519e+00, -2.2727e+00, -2.6877e-02,  2.0114e+00, -7.4357e-01,\n",
       "         -1.0000e+09],\n",
       "        [-2.2542e+00,  8.6703e-01,  3.9961e-01, -7.1266e-01, -2.0102e-02,\n",
       "         -3.9760e-01]])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = torch.tensor(mask, dtype=torch.float32) + fr_embed_tensor\n",
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Applying softmax to get attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax (vector to prob)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = torch.exp(x - torch.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/jcq2q4656l14lwb2cdw1pkfm0000gn/T/ipykernel_31576/1679037461.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(attention, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4724, 0.5276, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3811, 0.0685, 0.5504, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0893, 0.0215, 0.2753, 0.6139, 0.0000, 0.0000],\n",
       "        [0.0189, 0.0112, 0.1058, 0.8124, 0.0517, 0.0000],\n",
       "        [0.0172, 0.3890, 0.2437, 0.0801, 0.1602, 0.1098]])"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled)\n",
    "torch.tensor(attention, dtype=torch.float32)\n",
    "\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Computing weighted sum of embeddings using attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/jcq2q4656l14lwb2cdw1pkfm0000gn/T/ipykernel_31576/99769784.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(out, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8414e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-4.9545e-01, -4.7240e+08, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-1.2436e-01, -3.8110e+08, -4.4957e+08, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-5.7052e-01, -8.9330e+07, -1.1080e+08, -3.8608e+08, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-9.2311e-01, -1.8853e+07, -3.0054e+07, -1.3588e+08, -9.4832e+08,\n",
       "         -1.0000e+09],\n",
       "        [-1.3912e-01, -1.7154e+07, -4.0612e+08, -6.4985e+08, -7.2999e+08,\n",
       "         -8.9018e+08]])"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.matmul(attention, scaled)\n",
    "torch.tensor(out, dtype=torch.float32)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Add and Norm\n",
    "\n",
    "- Adding residual connection --> to ensure that there is a stronger information signal that flows through the network\n",
    "\n",
    "- Normalizing --> needed to prevent vanishing gradient during backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/jcq2q4656l14lwb2cdw1pkfm0000gn/T/ipykernel_31576/2677493677.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(out_with_residual, dtype = torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6827e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [ 2.1421e-01, -4.7240e+08, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [ 8.3635e-01, -3.8110e+08, -4.4957e+08, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-1.6875e+00, -8.9330e+07, -1.1080e+08, -3.8608e+08, -1.0000e+09,\n",
       "         -1.0000e+09],\n",
       "        [-2.6751e+00, -1.8853e+07, -3.0054e+07, -1.3588e+08, -9.4832e+08,\n",
       "         -1.0000e+09],\n",
       "        [-2.3933e+00, -1.7154e+07, -4.0612e+08, -6.4985e+08, -7.2999e+08,\n",
       "         -8.9018e+08]])"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember fr_embed_tensor = torch.tensor((fr_embed + fr_pos_encod), dtype = torch.float32)\n",
    "\n",
    "out_with_residual = out + fr_embed_tensor\n",
    "torch.tensor(out_with_residual, dtype = torch.float32)\n",
    "\n",
    "out_with_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm\n",
    "layer_norm = torch.nn.LayerNorm(d_model)\n",
    "normalized_out = layer_norm(out_with_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2361, -0.4472, -0.4472, -0.4472, -0.4472, -0.4472],\n",
       "        [ 1.9360,  0.7091, -0.6613, -0.6613, -0.6613, -0.6613],\n",
       "        [ 1.6469,  0.6638,  0.4872, -0.9327, -0.9327, -0.9327],\n",
       "        [ 1.0281,  0.8151,  0.7638,  0.1072, -1.3571, -1.3571],\n",
       "        [ 0.8083,  0.7654,  0.7400,  0.4994, -1.3478, -1.4653],\n",
       "        [ 1.3106,  1.2605,  0.1249, -0.5868, -0.8207, -1.2884]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Feed-Forward Network\n",
    " - a feed-forward network (FFN) that consists of two linear layers with a ReLU activation in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3263,  0.4261, -0.6664,  0.5714,  1.0014, -0.1260],\n",
       "        [-0.2389,  0.2112, -0.6452,  0.4037,  0.9688, -0.1275],\n",
       "        [-0.2358,  0.1036, -0.7250,  0.3573,  0.7150, -0.3568],\n",
       "        [-0.2452,  0.0290, -0.7106,  0.2980,  0.2706, -0.3207],\n",
       "        [-0.2525,  0.0448, -0.6882,  0.3072,  0.1719, -0.2632],\n",
       "        [-0.2428, -0.0014, -0.7032,  0.2823,  0.7057, -0.1947]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = nn.Sequential(\n",
    "    nn.Linear(d_model, 4 * d_model),  # Expand the model dimension\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4 * d_model, d_model)   # Project back to original size\n",
    ")\n",
    "\n",
    "ffn_out = ffn(normalized_out)\n",
    "ffn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: This completes one Transformer Decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8568, -0.1767, -1.3273, -0.0237,  0.4291, -0.7582],\n",
       "        [ 1.5805,  0.8139, -1.3832, -0.3483,  0.2093, -0.8723],\n",
       "        [ 1.6254,  0.8963, -0.2427, -0.6251, -0.2198, -1.4341],\n",
       "        [ 0.9436,  1.0079,  0.1752,  0.5458, -1.0250, -1.6476],\n",
       "        [ 0.6741,  0.9304,  0.1664,  0.9268, -1.0705, -1.6271],\n",
       "        [ 1.1568,  1.3591, -0.5846, -0.2949, -0.0945, -1.5418]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn_out_with_residual = ffn_out + normalized_out  # Skip connection\n",
    "normalized_ffn_out = layer_norm(ffn_out_with_residual)\n",
    "\n",
    "normalized_ffn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Multi-Head Self-Attention for Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim = d_model, num_heads = 2, batch_first = True)\n",
    "encoder_output, _ = multihead_attn(en_embeddings_with_position, en_embeddings_with_position, en_embeddings_with_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = layer_norm(encoder_output + en_embeddings_with_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ffn_out = ffn(encoder_output)\n",
    "encoder_ffn_out = layer_norm(encoder_ffn_out + encoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4807,  1.3482, -0.6100, -0.5612, -1.1166,  1.4204],\n",
       "        [-0.6194, -0.0743, -1.7432,  0.1996,  1.2402,  0.9972],\n",
       "        [ 0.7156, -0.9385, -0.9282,  1.2744, -1.0897,  0.9663],\n",
       "        [-0.3832, -1.3100, -0.6968,  1.3004, -0.2818,  1.3714],\n",
       "        [-1.3030, -0.9353,  0.5255,  1.0738, -0.6276,  1.2666]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_ffn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: Cross-Attention in Decoder\n",
    " - The decoder attends to both its own tokens (masked self-attention) and the encoder's output (cross-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=2, batch_first=True)\n",
    "\n",
    "cross_attn_output, _ = cross_attn(normalized_ffn_out, encoder_ffn_out, encoder_ffn_out)\n",
    "cross_attn_output = layer_norm(cross_attn_output + normalized_ffn_out)  # Residual connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_ffn_out = ffn(cross_attn_output)\n",
    "cross_ffn_out = layer_norm(cross_ffn_out + cross_attn_output)  # Add & Norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1518, -0.1076, -1.6454,  0.4414,  1.0145, -0.8547],\n",
       "        [ 1.0513,  0.6750, -1.6408,  0.1474,  0.8050, -1.0379],\n",
       "        [ 1.3299,  0.7086, -0.6739, -0.0926,  0.4724, -1.7445],\n",
       "        [ 0.7247,  0.8482, -0.1542,  1.0551, -0.7086, -1.7653],\n",
       "        [ 0.4279,  0.7598, -0.1508,  1.3898, -0.8005, -1.6262],\n",
       "        [ 0.8792,  1.0881, -0.9179,  0.1834,  0.4807, -1.7134]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_ffn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_ffn_out = ffn(cross_attn_output)\n",
    "cross_ffn_out = layer_norm(cross_ffn_out + cross_attn_output)  # Add & Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_projection = nn.Linear(d_model, len(vocab_fr))  # Map d_model → vocab_size\n",
    "logits = final_projection(cross_ffn_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Convert target sentence to tensor (ignoring SOS/EOS)\n",
    "target_tensor = torch.tensor([vocab_fr[word] for word in fr_tokens if word in vocab_fr], dtype=torch.long)\n",
    "\n",
    "# Ensure correct shape for loss calculation\n",
    "loss = loss_fn(logits.view(-1, len(vocab_fr)), target_tensor.view(-1))  # Flatten\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(ffn.parameters()) + list(cross_attn.parameters()) + list(final_projection.parameters()), lr=0.001)\n",
    "\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()  # Clear gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted indices shape: torch.Size([6])\n",
      "Predicted French Translation: un gros gros c'est gros\n"
     ]
    }
   ],
   "source": [
    "# Get predicted word indices\n",
    "predicted_indices = torch.argmax(logits, dim=-1)  # Choose highest probability index\n",
    "\n",
    "# Reverse mapping from index to word\n",
    "idx_to_word_fr = {idx: word for word, idx in vocab_fr.items()}\n",
    "\n",
    "# Check shape before proceeding\n",
    "print(\"Predicted indices shape:\", predicted_indices.shape)  \n",
    "\n",
    "# Ensure predicted_indices is a list of indices, not a single integer\n",
    "if predicted_indices.dim() == 1:  # If it's a single sequence\n",
    "    predicted_indices_list = predicted_indices.tolist()\n",
    "elif predicted_indices.dim() == 2:  # If batch dimension exists\n",
    "    predicted_indices_list = predicted_indices[0].tolist()\n",
    "else:\n",
    "    raise ValueError(\"Unexpected shape for predicted_indices:\", predicted_indices.shape)\n",
    "\n",
    "# Convert indices to words\n",
    "predicted_sentence = [idx_to_word_fr[idx] for idx in predicted_indices_list]\n",
    "\n",
    "# Remove special tokens\n",
    "translated_sentence = \" \".join([word for word in predicted_sentence if word not in [SOS, EOS]])\n",
    "\n",
    "print(\"Predicted French Translation:\", translated_sentence)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
